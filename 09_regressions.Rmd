# Регрессионный анализ

```{css, echo=FALSE}
.spoiler {
  visibility: hidden;
  background: #ffffe0
}

.spoiler::before {
  visibility: visible;
  background: cyan;
  content: "Спойлер! Наведите, чтобы увидеть ответ"
}

.spoiler:hover {
  visibility: visible;
}

.spoiler:hover::before {
  display: none;
}
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
```

## Введение

Дисклеймер: в этом файле под некоторыми заданиями есть текстовые ответы (без кода), скрытые под синенькой плашкой "спойлер". Она срабатывает при наведённом на неё курсоре:
  
[Если вы случайно проспойлерили себе ответ, не расстраивайтесь, вам всё ещё нужно написать код, который будет выдавать этот ответ :)]{.spoiler}

Итак, регрессия. Это метод, позволяющий исследовать взаимосвязь между одной зависимой переменной $y_i$ и одной или несколькими независимыми переменными $x_i$. 

В этом материале мы разберём, как построить парную и множественную линейные регрессии в R и что потом со всем этим делать.

## Парная линейная регрессия

## Мотематика

![](https://sun9-55.userapi.com/zAw1CCGDk5UIXXx-C2ljiEibLooC5dDXPDSBXQ/Zw9PIkO2Eb4.jpg)

В основе регрессионного анализа лежит метод наименьших квадратов. Формула регрессии выглядит так:

$$y_i = \beta_0 + \beta_1 x_i + \epsilon_i$$

где $\beta_0$ и $\beta_1$ -- это коэффициенты регрессии, неизвестные параметры, которые мы хотим найти, а $\epsilon_i$ -- ошибка, или расстояние от настоящего значения $y_i$ до предсказанного. 

Предсказанный игрек обозначается так: $\hat{y}_i$

А считается так:

$$\hat{y}_i = \beta_0 + \beta_1 x_i$$
Мы хотим оценить неизвестные беты и провести регрессионную прямую так, чтобы минимизировать суммарную ошибку. 

![](https://miro.medium.com/max/1050/0*QG8dIxNTaBH7Qrxq)

На картинке ниже ошибки -- это вертикальные красные линии -- расстояния до прямой. Если просто их суммировать, легко получить нулевую ошибку, т.к. эти расстояния имеют разные знаки. Поэтому придумали возводить ошибку в квадрат и искать такие параметры регрессии, которые минимизировали бы сумму квадратов ошибок -- это и есть метод наименьших квадратов (МНК).

![](https://risk-engineering.org/static/img/large/OLS.png)

Из уравнения регрессии можно выразить формулу ошибки

$$\epsilon_i = y_i - \beta_0 - \beta_1 x_i \\ $$
Откуда сумма квадратов ошибок:

$$\sum \epsilon_i^2 = \sum(y_i - \beta_0 - \beta_1 x_i)^2 \\ $$

Далее эту штуку можно минимизировать по параметрам $\beta$ (приравнять производную к нулю, и вот это вот всё) и вывести формулы для коэффициентов регрессии. Решение есть вот [тут](https://warwick.ac.uk/fac/soc/economics/staff/vetroeger/teaching/po906_week567.pdf) на страницах 7-9 (правда работает это только для случая парной регрессии)

![](https://www.meme-arsenal.com/memes/290f82ae820c34bda9a5e3c5ce4a65bc.jpg)

А мы переходим к разделу

### А как это будет в R

В R для линейной регрессии применяется функция `lm()` (linear model)

Попробуем на наших данных посмотреть на связь великолепности с размером обуви.

Как устроена функция `lm()`: удобнее всего прописать через запятую

* формулу в виде `y ~ x` (в случае множественной регрессии будет `y ~ x1 + x2` и так далее)
* данные в виде `data=df`

Можно и не выписывать данные отдельно, а написать через доллар `df$y ~ df$x` но когда у вас много переменных или датасет называется каким-то длинным словом, это будет уже тяжелее

```{r, include=FALSE}
df <- read.csv('data/about_us_eng.csv')
```


```{r, eval = FALSE}
df <- read.csv('about_us_eng.csv')
```


```{r}
lm(gorgeous~shoe_size, data=df)
```

Функция `lm()` выдаёт такую вот простенькую выдачу, где показана формула нашей регрессии и оценки коэффициентов $\beta_0$ (Intercept) и $\beta_1$ `shoe_size`.

Получается наша итоговая формула для предсказанного игрека выглядит так:

$$\hat{y}_i = 19.1977978 - 0.2881108 x_i$$

А вот наша регрессия на картиночке:

```{r echo=FALSE, message=FALSE}
ggplot(df, aes(shoe_size, gorgeous)) + 
  geom_point() +
  geom_smooth(method="lm")
```

Интерсепт отвечает за значение зависимой переменной в нуле: то есть при нулевом размере обуви великолепность человека будет аж 19 пунктов :)

Если вспомнить наш опрос, такого варианта ответа там не было, получается, наша регрессия предсказывает слишком большие значения? 

Не совсем. Если подставлять в формулу регрессии осмысленные значения размера обуви, то великолепность окажется в рамках нормы. Для среднестатистических взрослых людей наша регрессия предсказывает нормальные значения великолепности, но вообще в реальности проблема с выходом за рамки допустимых значений существует, и для некоторых специфически распределённых зависимых переменных линейная регрессия не подходит (например, для бинарного игрека используется логистическая, а чтобы ограничить значения предсказаний с одной стороны существует тобит-регрессия).


Но вернёмся к нашей модели. R позволяет записать регрессию в переменную, а потом посмотреть на более подробную выдачу с помощью комманды `summary()`

```{r}
model <- lm(gorgeous~shoe_size, data=df)
summary(model)
```

Что у нас тут есть:

Во-первых, добавились p-value и значимость коэффициентов. Что это такое?

Значимость -- это результат проверки гипотезы о равенстве коэффициента нулю. Эта гипотеза проверяется с помощью t-теста, который мы обсуждали в материале про статистические критерии.

Нулевая гипотеза: коэффициент бета равен нулю, то есть между переменными $x$ и $y$ нет взаимосвязи. Значим ли интерсепт, в большинстве случаев неважно (разве что вам принципиально знать, что люди с нулевым размером обуви имеют ненулевую великолепность))

p-value показывает вероятность отвергнуть верную нулевую гипотезу, то есть найти связь там где её нет. Чем оно меньше, тем лучше. Звёздочки показывают, что p-value меньше одного из трёх конвенциональных значений: 0.05, 0.01 и 0.001. А наши звёздочки говорят нам, что коэффициент $\beta_1$ значим на уровне 0.05, то есть по нашим данным можно сказать, что есть значимая отричательная связь между размером обуви и великолепностью.


### Подставляем значения в формулу регрессии

Что значит "подставлять в формулу регрессии осмысленные значения"? Формула, которую мы увидели выше, описывает линейную взаимосвязь между x и y. Соответственно, если подставить какой-нибудь предиктор икс ($x_i$) в формулу и подсчитать, то результат ($\hat{y}_i$) это предсказанное значение. Например: если мы взяли с улицы человека и спросили, какой у него размер обуви равен 39, то, согласно предсказаниям нашей модели, великолепность равна 7.96:


```{r}
19.1977978 - 0.2881108*39
```

Значения параметров 19.1977978 и 0.2881108 мы взяли из результата работы функции `lm()`. Но, конечно, считать руками это не путь воина (то есть, надо быть готовым/ой это сделать, но сначала всегда пытаться избежать))). 

Во-первых, коэффициенты модели можно достать вот так:

```{r}
model$coefficients
```

А главное, можно использовать функцию `predict()`. Она принимает на вход, во-первых, нашу модель (результат функции `lm()`, сохранённый в переменную). Если дать ей только нашу модель и ничего больше, она вернёт предсказанное значение для каждый строки (=каждого респондента) в нашем датасете:

```{r}
predict(model)
```

А можно ещё сделать новую таблицу с данными, дать её на вход в аргументе `newdata`, и тогда `predict()` воспользуется выведенной формулой, чтобы предсказать значения для этих данных. Важно! Названия колонки в новой таблице и в той, которую вы использовали в lm(), должны совпадать.

```{r}
newdf <- data.frame(shoe_size = 33) #создадим новую таблицу с одной колонкой shoe_size и значением 33.
predict(model, newdata = newdf)
```

Вот такое значение замечательности предсказывает наша модель для людей, у которых 33 размер обуви.

### Количество объяснённой дисперсии

Важный показатель того, насколько наша модель хорошо объясняет данные, это $R^2$, или процент объяснённой дисперсии. Мы можем увидить его внизу выдачи `summary(model)`:
```{r}
summary(model)

```

Или вытащить из `summary(model)` с помощью доллара:
```{r}
summary(model)$r.squared
```

$R^2$ показывает, какой процент дисперсии - то есть, шума в данных - объясняется нашим предиктором. В нашей модели размер обуви объясняет примерно 
```{r}
paste0(round(summary(model)$r.squared, 2)*100, '%')
```

дисперсии в оценке великолепности. В общем и целом модель с более высоким $R^2$ лучше, но при этом модель с б*о*льшим количеством предикторов будет всегда по умолчанию иметь б*о*льший $R^2$, так что сравнивать с помощью $R^2$ разные модели мы не советуем. 


```{block, type = "rmdtask"}
ЗАДАЧКА

Посчитайте предсказанную великолепность людей с минимальным и максимальным размером обуви
```

[Минимальные и максимальные размеры -- 35 и 45, а ответы -- 9.113921 и 6.232813  соответственно]{.spoiler}

![Поздравляем! Вы построили свою первую регрессию и проинтерпретировали ВООБЩЕ ВСЕ в выдаче (ну почти))](https://cdn.ebaumsworld.com/thumbs/natural/gallery/2242139/86407354.jpg)


### Посмотреть на остатки

Одно из базовых допущений линейной регрессии - что остатки, или ошибки, распределены нормально. Остатки это разница между реальным значением зависимой переменной (здесь великолепности) и значением, которое предсказывает наша модель. Остатки можно "вынуть" из объекта с моделью:

```{r}
model$residuals
```

То есть, здесь каждое число это разница между реальным великолепием каждого респондента, и великолепием, которое предсказывает наша модель :))

Мы можем построить уже дорогую и знакомую нам гистограмму, чтобы проверить, как у них там с нормальностью:

```{r}
hist(model$residuals)
```
Или проверить их на нормальность с помощью формального теста, например, Шапиро-Уилкс:
```{r}
shapiro.test(model$residuals)
```

Здесь мои графические выводы (да норм) и вывод Шапиро-Уилка совпадают. Но на самом деле на больших выборках (хотя бы >30) линейная регрессия устойчива к не нормально распределённым данным. Если у вас на гистограмме симметричное распределение с одним пиком, то вы в безопасности.

![](https://moidrujok.ru/wp-content/uploads/2020/04/eto-norma.jpg)

О других допущениях можно почитать вот [туть](https://www.statology.org/linear-regression-assumptions/)

```{block, type = "rmdtask"}
Придумайте гипотезу о взаимосвязи двух количественных переменных в нашем датасете. Проверьте эту гипотезу с помощью линейной регрессии :) Проверьте остатки на нормальность.
```


## Другие виды предикторов и их комбинации

### Категориальный предиктор

Можно построить регрессию $y$ на одну категориальную переменную.

Давайте попробуем предсказать связь великолепности с наличием кошки (создадим для этого переменную "cat_owner") 

```{r}
df$cat_owner <- ifelse(df$cats != 0, 'yes','no')
model_cat = lm(data=df, gorgeous~cat_owner)
summary(model_cat)
```

При наличии категориального предиктора R создает k-1 дамми-переменную, каждая из которых принимает значение 1 в своей категории и 0 во всех остальных. 

То есть в случае с нашими кошками в данных существует две категории: "есть кошки" и "нет кошек". Значит в регрессии будет одна дамми-переменная, которая равна единице, если кошки есть. 

Смысл этого, возможно, будет более понятен из формул, но если что, про дамми можно почитать вот например [здесь](https://www.displayr.com/what-are-dummy-variables/)

В summary значения категорий приписываются к названию изначальной переменной -- в нашем примере получилось "cat_owneryes". Посмотрим еще раз как это выглядит в формуле:

$$\hat{y}_i = \beta_0 + \beta_1 x_i$$
Помним, что $x_i$ принимает значение 1 для кошатников и 0 для не-кошатников. Соответственно, для кошатников уравнение будет выглядеть так: 

$\hat{y}_i = \beta_0 + \beta_1 \cdot 1 \Leftrightarrow \hat{y}_i = \beta_0 + \beta_1$, 

а для не-кошатников: 

$\hat{y}_i = \beta_0 + \beta_1 \cdot 0 \Leftrightarrow \hat{y}_i = \beta_0$

Сами коэффициенты в выдаче интерпретируются так: средняя великолепность не-кошатников равна 6.6667 (intercept), а средняя великолепность кошатников на 1.7833 больше (1.7833 + 6.6667 = 8.45). Если обратить внимание на значимость, кошатники в среднем *значимо* более великолепны, чем не-кошатники.

Если же мы вставим в регрессию первоначальный предиктор про кошек, получится вот что:

```{r}
model_cat2 = lm(data=df, gorgeous~cats)
summary(model_cat2)
```
И здесь все категории нужно интерпретировать *по отдельности, в сравнении с базовой* (в нашем случае базовая категория -- 0 кошек). 

Так например, люди с одной кошкой в среднем на 1.64 более великолепны, чем люди без кошек, и этот предиктор значим на уровне 0.1. А люди с двумя кошками на 1.83 более великолепны, чем люди без кошек, но разница между ними незначима.

```{block, type = "rmdtask"}
ЗАДАЧКА

Постройте регрессию роста на переменную "выбор горячего напитка". Какой напиток пьют самые высокие в среднем люди?
```

![](https://i.forfun.com/jsk254e2.jpeg)

### Количественный и категориальный предиктор вместе

Самый главный кайф регрессии в том, что в качестве предикторов можно засунуть много переменных сразу. Давайте для начала засунем две -- "размер обуви" и "есть кошка"

```{r}
model_num_cat <- lm(data=df, gorgeous ~ shoe_size + cat_owner)
summary(model_num_cat)
```

Судя по выдаче, предиктор "размер обуви" перестал быть значимым совсем, то есть изменение размера обуви на единицу никак не сказывается на изменении великолепности.

В то же время люди, имеющие кошек, по-прежнему значимо более великолепны, чем не имеющие кошек.

А вот как наши точки и регрессионные прямые будут выглядеть на графике:

```{r echo=FALSE, message=FALSE}
ggplot(df, aes(shoe_size, gorgeous, color=cat_owner)) + 
  geom_point() +
  moderndive::geom_parallel_slopes()
```

Это параллельные регрессионные прямые. Они отличаются только на параметр $\beta_0$

Чтобы разрешить прямым не быть параллельными, можно добавить взаимодействие!

Формула внутри `lm` поменяется: между предикторами будет не плюс, а умножение:

```{r}
model_inter_cut <- lm(data=df, gorgeous ~ shoe_size * cat_owner)
summary(model_inter_cut)
```

Коэффициентов  стало уже даже больше, чем переменных!

Давайте по порядку. Размер обуви снова стал значимым. Но не для всех! Только для группы не-кошатников. Среди не-кошатников увеличение размера обуви на единицу связано со снижением великолепности на 0.4. 

Следующий предиктор -- наличие кошки. Это дополнительный интерсепт для кошатников. Он означает, что их личная регрессионная прямая будет выходить из точки на 13 единиц ниже. Другими словами, это разница между кошатниками и не-кошатниками *с нулевым размером обуви* (в нашем случае незначимая).

Регрессионная прямая у кошатников тоже будет личная, ее наклон будет таким: -0.41 + 0.38 = -0.03 (коэфф. не-кошатников плюс коэфф. кошатников). То есть личная линия кошатников будет практически параллельна оси икс.

Проверим на картинке:

```{r echo=FALSE, message=FALSE}
ggplot(df, aes(y=gorgeous,x= shoe_size, color=cat_owner)) + 
  geom_smooth(method='lm') + 
  geom_point()
```

Всё как мы и предсказывали! Личная линия кошатников (синяя) начинается *ниже* линии не-кошатников (если мысленно продлить обе линии влево до нуля по оси икс), и при этом она более пологая.

Красная же линия, наоборот, получилась более наклонной, чем в регрессии без взаимодействия, и во всей регрессионной модели только один этот наклон оказался значимым (ну и еще интерсепт не-кошатников).

Вспомним, что когда мы строили регрессию на всей выборке, мы получили значимую отрицательную взаимосвязь между размером обуви и великолепностью. Теперь, посмотрев на регрессионные прямые внутри групп, можем сделать вывод, что тот отрицательный эффект был обусловлен только не-кошатниками, в группе кошатников он не выявляется.

```{block, type = "rmdtask"}
ЗАДАЧКА

Проверьте связь между великолепностью и размером обуви для групп: 0 или 1 родственник vs. 2 и более родственников. В какой группе связь положительная? Насколько именно увеличится великолепность с увеличением размера обуви на 1? 
```

[У людей, которые имеют больше одного брата или сестры, великолепность увеличивается вместе с размером обуви примерно на 0.3.]{.spoiler}

![Поздравляю! Это почти совсем всё про регрессии!](https://gifki.info/uploads/posts/2017-01/1485245017_39-pozdravlyayu-vseh.gif)

## Два количественных предиктора

Ну и наконец мы можем добавить в регрессию два (а то и больше!) количественных предикторов. Давайте попробуем предсказать размер обуви по длине волос и росту.

```{r}
model_num <- lm(data=df, shoe_size ~ hair_length + height)
summary(model_num)
```

Ура, снова нормальное количество коэффициентов :)

Проинтерпретируем же их:

* при увеличении длины волос на сантиметр размер обуви в среднем уменьшается на 0.047 *при фиксированном значении роста*. То есть среди людей *с одинаковым ростом* те, у кого длиннее волосы, будут в среднем иметь чуть меньший размер обуви. Эта связь очень слабо значима, только на уровне 0.1

* при увеличении роста на сантиметр размер обуви в среднем увеличивается на 0.23 *при фиксированном значении длины волос*. Эта связь значима на уровне 0.001; другими словами, размер обуви сильно связан с ростом

Ну и напоследок:

```{block, type = "rmdtask"}
ФИНАЛЬНАЯ ЗАДАЧЕЧКА

Загрузите по ссылкам два датесета:

dfmale <- read.csv('https://people.sc.fsu.edu/~jburkardt/datasets/triola/mhealth.csv')

dffemale <- read.csv('https://people.sc.fsu.edu/~jburkardt/datasets/triola/fhealth.csv')

Это два датасета с результатами прохождения медицинского обследования для мужчин и женщин. В них одинаковые по смыслу колонки, но с немного разными названиями. Первая колонка -- это просто айди каждого человека. 

Для начала придумайте, как их вертикально склеить. Затем попробуйте найти модель, которая с наилучшим R-квадратом будет предсказывать, например, уровень холестерина. Финальную модель проверьте на нормальность остатков.
```


![До встречи на хакатоне!](https://www.meme-arsenal.com/memes/26113082cacca6345d17bc4e159f8de6.jpg)
